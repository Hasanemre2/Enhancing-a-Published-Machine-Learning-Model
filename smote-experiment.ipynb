{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d236040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# SMOTE EXPERIMENT (SEPARATE FILE)\n",
    "# =========================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2c87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# LOAD DATASETS\n",
    "# =========================================================\n",
    "datasets = [\n",
    "    pd.read_csv(\"Youtube01-Psy.csv\"),\n",
    "    pd.read_csv(\"Youtube02-KatyPerry.csv\"),\n",
    "    pd.read_csv(\"Youtube03-LMFAO.csv\"),\n",
    "    pd.read_csv(\"Youtube04-Eminem.csv\"),\n",
    "    pd.read_csv(\"Youtube05-Shakira.csv\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f68601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hatice\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TEXT CLEANING\n",
    "# =========================================================\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    tokens = [w for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "for df in datasets:\n",
    "    df[\"clean_content\"] = df[\"CONTENT\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776dd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# EXTRA FEATURES\n",
    "# =========================================================\n",
    "def add_extra_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"comment_len\"] = df[\"CONTENT\"].astype(str).apply(len)\n",
    "    df[\"url_count\"] = df[\"CONTENT\"].str.count(\"http\")\n",
    "    df[\"excl_count\"] = df[\"CONTENT\"].str.count(\"!\")\n",
    "    df[\"upper_ratio\"] = df[\"CONTENT\"].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c.isupper()) / (len(str(x)) + 1)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "datasets = [add_extra_features(df) for df in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929e4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# MODEL EVALUATION FUNCTION (SAME AS BASELINE)\n",
    "# =========================================================\n",
    "def run_all_models(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
    "        \"Bernoulli NB\": BernoulliNB(),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"SVM Linear\": LinearSVC(dual=False, max_iter=2000, random_state=42),\n",
    "        \"Bagging DT\": BaggingClassifier(\n",
    "             estimator=DecisionTreeClassifier(random_state=42),\n",
    "            n_estimators=20,\n",
    "           random_state=42\n",
    "        ),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "        \"Stacking\": StackingClassifier(\n",
    "            estimators=[\n",
    "               (\"dt\", DecisionTreeClassifier(max_depth=5)),\n",
    "                (\"nb\", BernoulliNB())\n",
    "         ],\n",
    "           final_estimator=LogisticRegression()\n",
    "         )\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        results[name] = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred) * 100,\n",
    "            \"SC\": (tp / (tp + fn) * 100) if (tp + fn) > 0 else 0,\n",
    "            \"BH\": (fp / (tn + fp) * 100) if (tn + fp) > 0 else 0,\n",
    "            \"F1\": f1_score(y_test, y_pred) ,\n",
    "            \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08c0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# TRAIN + SMOTE / TEST (REAL DATA)\n",
    "# =========================================================\n",
    "def prepare_train_test_with_smote(df, target_size, test_size=0.30, random_state=42):\n",
    "\n",
    "    X_text = df[\"clean_content\"]\n",
    "    y = df[\"CLASS\"]\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=3000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "        max_df=0.85,\n",
    "        stop_words=\"english\",\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "\n",
    "    X_tfidf = tfidf.fit_transform(X_text)\n",
    "\n",
    "    extra = csr_matrix(\n",
    "        df[[\"comment_len\", \"url_count\", \"excl_count\", \"upper_ratio\"]].values\n",
    "    )\n",
    "\n",
    "    X = hstack([X_tfidf, extra])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy={0: target_size, 1: target_size},\n",
    "        random_state=random_state,\n",
    "        k_neighbors=3\n",
    "    )\n",
    "\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_train_smote, X_test, y_train_smote, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b901837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# RUN SMOTE EXPERIMENTS (1000 → 5000)\n",
    "# =========================================================\n",
    "def run_smote_experiments(df, sizes=[1000,5000]):\n",
    "    results = {}\n",
    "\n",
    "    for size in sizes:\n",
    "        print(f\"\\nRunning SMOTE with {size} samples per class\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = prepare_train_test_with_smote(\n",
    "            df,\n",
    "            target_size=size\n",
    "        )\n",
    "\n",
    "        results[size] = run_all_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9140103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ DATASET 1 ================\n",
      "\n",
      "Running SMOTE with 1000 samples per class\n",
      "\n",
      "Running SMOTE with 5000 samples per class\n",
      "DEBUG sizes: dict_keys([1000, 5000])\n",
      "\n",
      "================ DATASET 2 ================\n",
      "\n",
      "Running SMOTE with 1000 samples per class\n",
      "\n",
      "Running SMOTE with 5000 samples per class\n",
      "DEBUG sizes: dict_keys([1000, 5000])\n",
      "\n",
      "================ DATASET 3 ================\n",
      "\n",
      "Running SMOTE with 1000 samples per class\n",
      "\n",
      "Running SMOTE with 5000 samples per class\n",
      "DEBUG sizes: dict_keys([1000, 5000])\n",
      "\n",
      "================ DATASET 4 ================\n",
      "\n",
      "Running SMOTE with 1000 samples per class\n",
      "\n",
      "Running SMOTE with 5000 samples per class\n",
      "DEBUG sizes: dict_keys([1000, 5000])\n",
      "\n",
      "================ DATASET 5 ================\n",
      "\n",
      "Running SMOTE with 1000 samples per class\n",
      "\n",
      "Running SMOTE with 5000 samples per class\n",
      "DEBUG sizes: dict_keys([1000, 5000])\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FINAL RUN\n",
    "# =========================================================\n",
    "\n",
    "all_smote_results = {}\n",
    "\n",
    "for i, df in enumerate(datasets):\n",
    "    print(f\"\\n================ DATASET {i+1} ================\")\n",
    "\n",
    "    res = run_smote_experiments(df)\n",
    "    print(\"DEBUG sizes:\", res.keys())   # ← ÇOK ÖNEMLİ\n",
    "\n",
    "    all_smote_results[f\"Dataset_{i+1}\"] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba1d4e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================ TABLE (1000 spam + 1000 ham) ================\n",
      "DEBUG table count: 5\n",
      "                     Accuracy       SC      BH      F1     MCC\n",
      "AdaBoost              89.6075  80.8401  1.3832  0.8861  0.8060\n",
      "Bagging DT            92.1361  87.7163  3.4221  0.9172  0.8470\n",
      "Bernoulli NB          88.5103  80.4180  3.2972  0.8697  0.7877\n",
      "Decision Tree         91.5327  88.5500  5.5873  0.9117  0.8323\n",
      "Logistic Regression   91.7839  87.8951  4.4825  0.9142  0.8386\n",
      "Random Forest         94.0093  89.8182  1.7606  0.9364  0.8839\n",
      "SVM Linear            93.3573  91.2537  4.5815  0.9324  0.8678\n",
      "Stacking              92.8374  89.4721  3.6201  0.9260  0.8592\n",
      "\n",
      "\n",
      "================ TABLE (5000 spam + 5000 ham) ================\n",
      "DEBUG table count: 5\n",
      "                     Accuracy       SC      BH      F1     MCC\n",
      "AdaBoost              90.1687  81.7237  1.0553  0.8932  0.8166\n",
      "Bagging DT            92.7109  88.4970  3.0831  0.9239  0.8572\n",
      "Bernoulli NB          90.1197  83.9970  3.6746  0.8925  0.8144\n",
      "Decision Tree         91.7265  88.3826  4.9316  0.9138  0.8363\n",
      "Logistic Regression   92.8775  90.0999  4.4825  0.9269  0.8591\n",
      "Random Forest         93.6993  88.7672  1.3721  0.9328  0.8795\n",
      "SVM Linear            92.8638  92.3046  6.6971  0.9290  0.8579\n",
      "Stacking              92.8271  90.1384  4.3253  0.9267  0.8583\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# AGGREGATE RESULTS (ONLY TABLE 6 & 7)\n",
    "# =========================================================\n",
    "\n",
    "final_tables = {1000: [], 5000: []}\n",
    "\n",
    "for dataset_name, size_results in all_smote_results.items():\n",
    "    for size in [1000, 5000]:\n",
    "        if size in size_results:\n",
    "            final_tables[size].append(size_results[size])\n",
    "\n",
    "for size in [1000, 5000]:\n",
    "    print(f\"\\n\\n================ TABLE ({size} spam + {size} ham) ================\")\n",
    "\n",
    "    print(\"DEBUG table count:\", len(final_tables[size]))\n",
    "\n",
    "    if len(final_tables[size]) == 0:\n",
    "        print(\"❌ NO DATA FOUND FOR THIS SIZE\")\n",
    "        continue\n",
    "\n",
    "    combined = pd.concat(final_tables[size])\n",
    "    mean_table = combined.groupby(combined.index).mean()\n",
    "\n",
    "    print(mean_table.round(4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
