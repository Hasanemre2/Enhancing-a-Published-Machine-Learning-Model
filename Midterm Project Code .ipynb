{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10560307",
      "metadata": {
        "id": "10560307"
      },
      "source": [
        "**MIDTERM PROJECT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6861e2b1",
      "metadata": {
        "id": "6861e2b1"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# IMPORTS\n",
        "# =========================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, confusion_matrix\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2f50fc84",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================================================\n",
        "# LOAD DATASETS\n",
        "# =========================================================\n",
        "datasets = [\n",
        "    pd.read_csv(\"Youtube01-Psy.csv\"),\n",
        "    pd.read_csv(\"Youtube02-KatyPerry.csv\"),\n",
        "    pd.read_csv(\"Youtube03-LMFAO.csv\"),\n",
        "    pd.read_csv(\"Youtube04-Eminem.csv\"),\n",
        "    pd.read_csv(\"Youtube05-Shakira.csv\")\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "71280a64",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Hatice\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================================================\n",
        "# TEXT CLEANING\n",
        "# =========================================================\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = [w for w in text.split() if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "for df in datasets:\n",
        "    df[\"clean_content\"] = df[\"CONTENT\"].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b5c2949a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================================================\n",
        "# EXTRA FEATURES\n",
        "# =========================================================\n",
        "def add_extra_features(df):\n",
        "    df = df.copy()\n",
        "    df[\"comment_len\"] = df[\"CONTENT\"].astype(str).apply(len)\n",
        "    df[\"url_count\"] = df[\"CONTENT\"].str.count(\"http\")\n",
        "    df[\"excl_count\"] = df[\"CONTENT\"].str.count(\"!\")\n",
        "    df[\"upper_ratio\"] = df[\"CONTENT\"].apply(\n",
        "        lambda x: sum(1 for c in str(x) if c.isupper()) / (len(str(x)) + 1)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "datasets = [add_extra_features(df) for df in datasets]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c16af3e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================================================\n",
        "# TF-IDF + SPLIT\n",
        "# =========================================================\n",
        "def prepare_train_test(df, test_size=0.30):\n",
        "    X_text = df[\"clean_content\"]\n",
        "    y = df[\"CLASS\"]\n",
        "\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=4000,\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=3,\n",
        "        max_df=0.85,\n",
        "        stop_words=\"english\",\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "\n",
        "    X = tfidf.fit_transform(X_text)\n",
        "\n",
        "    from scipy.sparse import hstack, csr_matrix\n",
        "    extra = csr_matrix(df[[\"comment_len\", \"url_count\", \"excl_count\", \"upper_ratio\"]].values)\n",
        "    X = hstack([X, extra])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b24ec0df",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================================================\n",
        "# MODEL FUNCTION\n",
        "# =========================================================\n",
        "def run_all_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "        \"Bernoulli NB\": BernoulliNB(),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "\n",
        "        \"SVM Linear\": SVC(kernel=\"linear\"),\n",
        "\n",
        "        \"Bagging DT\": BaggingClassifier(\n",
        "            estimator=DecisionTreeClassifier(random_state=42),\n",
        "            n_estimators=20,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        \"AdaBoost\": AdaBoostClassifier(\n",
        "            n_estimators=50,\n",
        "            random_state=42\n",
        "        ),\n",
        "\n",
        "        \"Stacking\": StackingClassifier(\n",
        "            estimators=[\n",
        "                (\"dt\", DecisionTreeClassifier(max_depth=5)),\n",
        "                (\"nb\", BernoulliNB())\n",
        "            ],\n",
        "            final_estimator=LogisticRegression()\n",
        "        )\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "        results[name] = {\n",
        "            \"Accuracy\": accuracy_score(y_test, y_pred) * 100,\n",
        "            \"SC\": (tp / (tp + fn) * 100) if (tp + fn) > 0 else 0,\n",
        "            \"BH\": (fp / (tn + fp) * 100) if (tn + fp) > 0 else 0,\n",
        "            \"F1\": f1_score(y_test, y_pred) * 100,\n",
        "            \"MCC\": matthews_corrcoef(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "548eaa2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# =========================================================\n",
        "# REPEATED EXPERIMENT (FIXED)\n",
        "# =========================================================\n",
        "def run_repeated_experiment(X, y, n_runs=5):\n",
        "    all_results = []\n",
        "\n",
        "    for i in range(n_runs):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y,\n",
        "            test_size=0.3,\n",
        "            random_state=42 + i,\n",
        "            stratify=y\n",
        "        )\n",
        "        all_results.append(run_all_models(X_train, X_test, y_train, y_test))\n",
        "\n",
        "    models = all_results[0].keys()\n",
        "    metrics = all_results[0][list(models)[0]].keys()\n",
        "\n",
        "    summary = {}\n",
        "    for model in models:\n",
        "        summary[model] = {}\n",
        "        for metric in metrics:\n",
        "            vals = [res[model][metric] for res in all_results]\n",
        "            summary[model][metric] = f\"{np.mean(vals):.2f} ± {np.std(vals):.2f}\"\n",
        "\n",
        "    return pd.DataFrame(summary).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2f780fd1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FINAL RESULTS\n",
            "\n",
            "                         Accuracy            SC           BH            F1  \\\n",
            "AdaBoost             92.19 ± 1.11  87.40 ± 2.59  3.03 ± 1.52  91.76 ± 1.30   \n",
            "Bagging DT           92.76 ± 2.05  89.27 ± 4.37  3.81 ± 1.71  92.42 ± 2.33   \n",
            "Bernoulli NB         94.10 ± 2.51  97.71 ± 1.87  9.50 ± 4.92  94.33 ± 2.35   \n",
            "Decision Tree        91.62 ± 2.72  90.81 ± 3.13  7.60 ± 3.16  91.52 ± 2.81   \n",
            "Logistic Regression  92.38 ± 2.63  87.38 ± 4.02  2.66 ± 1.50  91.91 ± 2.96   \n",
            "Random Forest        94.10 ± 1.11  93.88 ± 3.10  5.71 ± 1.24  94.04 ± 1.27   \n",
            "SVM Linear           95.81 ± 1.77  93.12 ± 3.78  1.52 ± 1.41  95.64 ± 1.94   \n",
            "Stacking             96.19 ± 0.85  95.43 ± 1.92  3.03 ± 2.26  96.16 ± 0.85   \n",
            "\n",
            "                             MCC  \n",
            "AdaBoost             0.85 ± 0.02  \n",
            "Bagging DT           0.86 ± 0.04  \n",
            "Bernoulli NB         0.89 ± 0.05  \n",
            "Decision Tree        0.83 ± 0.05  \n",
            "Logistic Regression  0.85 ± 0.05  \n",
            "Random Forest        0.88 ± 0.02  \n",
            "SVM Linear           0.92 ± 0.03  \n",
            "Stacking             0.92 ± 0.02  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================================================\n",
        "# FINAL RUN\n",
        "# =========================================================\n",
        "all_tables = []\n",
        "\n",
        "for df in datasets:\n",
        "    X, y = prepare_train_test(df)\n",
        "    all_tables.append(run_repeated_experiment(X, y))\n",
        "\n",
        "final_table = pd.concat(all_tables).groupby(level=0).agg(\"first\")\n",
        "\n",
        "print(\"\\nFINAL RESULTS\\n\")\n",
        "print(final_table)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
